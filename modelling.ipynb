{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_neg = pd.read_csv(\"data_bersih_negative.csv\")\n",
    "df_pos = pd.read_csv(\"data_bersih_positive.csv\")\n",
    "df_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>https://thinkquest.org</td>\n",
       "      <td>site cant reached thinkquestorg took long resp...</td>\n",
       "      <td>aman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>https://synonym.com</td>\n",
       "      <td>synonymcom new word condensation synonym anywa...</td>\n",
       "      <td>aman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>https://cliphunter.com</td>\n",
       "      <td>home categories popular pornstars playlists li...</td>\n",
       "      <td>bahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>https://mentalfloss.com</td>\n",
       "      <td>please note website includes accessibility sys...</td>\n",
       "      <td>aman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://elephanttube.com</td>\n",
       "      <td>bagus news elephanttube merged xxxcom closed p...</td>\n",
       "      <td>bahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>https://chaturbate.com</td>\n",
       "      <td>act masturbating chatting online status anonym...</td>\n",
       "      <td>bahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>https://cam.ac.uk</td>\n",
       "      <td>choices little files save device remember pref...</td>\n",
       "      <td>aman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>https://unt.edu</td>\n",
       "      <td>main find passion unt search enrollment soars ...</td>\n",
       "      <td>aman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://porntube.com</td>\n",
       "      <td>porn tube network tubeatube porner bros fux ho...</td>\n",
       "      <td>bahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>https://edublogs.org</td>\n",
       "      <td>schools districts sign blogs websites educatio...</td>\n",
       "      <td>aman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          url  \\\n",
       "311    https://thinkquest.org   \n",
       "301       https://synonym.com   \n",
       "72     https://cliphunter.com   \n",
       "219   https://mentalfloss.com   \n",
       "78   https://elephanttube.com   \n",
       "..                        ...   \n",
       "64     https://chaturbate.com   \n",
       "331         https://cam.ac.uk   \n",
       "321           https://unt.edu   \n",
       "26       https://porntube.com   \n",
       "282      https://edublogs.org   \n",
       "\n",
       "                                                  text  status  \n",
       "311  site cant reached thinkquestorg took long resp...    aman  \n",
       "301  synonymcom new word condensation synonym anywa...    aman  \n",
       "72   home categories popular pornstars playlists li...  bahaya  \n",
       "219  please note website includes accessibility sys...    aman  \n",
       "78   bagus news elephanttube merged xxxcom closed p...  bahaya  \n",
       "..                                                 ...     ...  \n",
       "64   act masturbating chatting online status anonym...  bahaya  \n",
       "331  choices little files save device remember pref...    aman  \n",
       "321  main find passion unt search enrollment soars ...    aman  \n",
       "26   porn tube network tubeatube porner bros fux ho...  bahaya  \n",
       "282  schools districts sign blogs websites educatio...    aman  \n",
       "\n",
       "[375 rows x 3 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Menggabungkan DataFrame df_neg dan df_pos\n",
    "df = pd.concat([df_neg, df_pos], ignore_index=True)\n",
    "\n",
    "# Mengacak urutan baris dalam DataFrame\n",
    "shuffled_indices = np.random.permutation(df.index)\n",
    "df = df.loc[shuffled_indices]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Akurasi :  0.9466666666666667\n",
      "Laporan Klasifikasi : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        aman       0.90      0.97      0.93        29\n",
      "      bahaya       0.98      0.93      0.96        46\n",
      "\n",
      "    accuracy                           0.95        75\n",
      "   macro avg       0.94      0.95      0.94        75\n",
      "weighted avg       0.95      0.95      0.95        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Pra-pemrosesan Teks\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(df['text'].apply(lambda x: np.str_(x)))\n",
    "y = df['status']\n",
    "\n",
    "# Pembagian Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pembuatan dan Pelatihan Model SVM\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluasi Model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Hasil Akurasi : \",accuracy)\n",
    "print(\"Laporan Klasifikasi : \\n\",classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Simpan TF-IDF Vectorizer\n",
    "with open('Model/tfidf_vectorizer_snailly.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "\n",
    "# Simpan model ke file PKL\n",
    "pkl_filename = 'Model/model_snaily.pkl'\n",
    "with open(pkl_filename, 'wb') as pkl_file:\n",
    "    pickle.dump(svm_model, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilitas Aman: [0.64116437]\n",
      "Probabilitas Berbahaya: [0.35883563]\n",
      "Prediksi: ['aman']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Teks baru yang akan diuji\n",
    "new_text = \"dadang jsn front page site home study materials elementary middle school classes, complete semester natural sciences, write comment, education friends happy learning opportunity time, natural sciences, SDMI accessed directly online used e-learning school home field study iv odd full chapter human skeleton, maintenance function, human function, influence body posture growth\"\n",
    "\n",
    "# Pra-pemrosesan teks baru\n",
    "# preprocessed_new_text = preprocess_text(new_text)\n",
    "new_text_vector = tfidf_vectorizer.transform([new_text])\n",
    "\n",
    "# Memuat kembali model dari file PKL\n",
    "loaded_svm_model = None\n",
    "with open('Model/model_snaily_1.pkl', 'rb') as pkl_file:\n",
    "    loaded_svm_model = pickle.load(pkl_file)\n",
    "\n",
    "# Gunakan model yang dimuat untuk membuat prediksi\n",
    "new_predictions = loaded_svm_model.predict(new_text_vector)\n",
    "\n",
    "# Gunakan model yang dimuat untuk mendapatkan signed distance dari hyperplane\n",
    "decision_value = loaded_svm_model.decision_function(new_text_vector)\n",
    "\n",
    "# Menerapkan Platt scaling untuk mengkonversi signed distance menjadi probabilitas\n",
    "a = loaded_svm_model.coef_[0]\n",
    "b = loaded_svm_model.intercept_\n",
    "prob_berbahaya = 1 / (1 + np.exp(-decision_value - b))\n",
    "prob_aman = 1 - prob_berbahaya\n",
    "\n",
    "print(\"Probabilitas Aman:\", prob_aman)\n",
    "print(\"Probabilitas Berbahaya:\", prob_berbahaya)\n",
    "# Prediksi menggunakan model SVM\n",
    "# prediction = svm_model.predict(new_text_vector)\n",
    "\n",
    "print(\"Prediksi:\", new_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
